# -*- coding: utf-8 -*-
"""Yet another copy of EnglishFakeNewsClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jvcfWRxouHh7D4uf2Cl4QtKPrAvVAJAD
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

true_path = '/content/drive/MyDrive/True.csv'
fake_path = '/content/drive/MyDrive/Fake.csv'

true_df = pd.read_csv(true_path)
fake_df = pd.read_csv(fake_path)

display(true_df)

display(fake_df)

"""# Data Visulization"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(y="subject", palette="coolwarm", data=true_df).set_title('True News Subject Distribution')
plt.show()

sns.countplot(y="subject", palette="coolwarm", data=fake_df).set_title('Fake News Subject Distribution')
plt.show()

"""## Real News Word Cloud"""

from wordcloud import WordCloud

real_titles = true_df.title
real_titles_ls = [text for text in real_titles]
# print(alls)
real_all_words = ' '.join(real_titles)
wordcloud_real = WordCloud(background_color='white',
    width= 800, height= 500,
    max_font_size = 180,
    collocations = False).generate(real_all_words)

plt.figure(figsize=(10,7))
plt.imshow(wordcloud_real, interpolation='bilinear')
plt.axis("off")
plt.show()

"""## Fake News Word Cloud"""

fake_titles = fake_df.title
fake_titles_ls = [text for text in fake_titles]
# print(alls)
fake_all_words = ' '.join(fake_titles)
wordcloud_fake = WordCloud(background_color='white',
    width= 800, height= 500,
    max_font_size = 180,
    collocations = False).generate(fake_all_words)

plt.figure(figsize=(10,7))
plt.imshow(wordcloud_fake, interpolation='bilinear')
plt.axis("off")
plt.show()

"""# Data Preprocessing

## Data Combination
"""

# Add Labels to both df
true_df['true'] = 1
fake_df['true'] = 0

# Concat
df = pd.concat([true_df, fake_df])
display(df)

"""## Inspect Lengths of News"""

titles = [text for text in df.title]

max_len = 0
titles_len = []
for title in titles:
    titles_len.append(len(title.split()))
    max_len = max(len(title.split()), max_len)

print('Number of titles:', len(titles))
print('Max length of the titles:', max_len)
print('Mean length of the titles:', np.mean(titles_len))

plt.figure(figsize=(20,5))
g = sns.countplot(x=titles_len)
g.set_xticklabels(g.get_xticklabels(), rotation=50)
plt.show()

texts = [text for text in df.text]

max_len = 0
texts_len = []
for text in texts:
    texts_len.append(len(text.split()))
    max_len = max(len(text.split()), max_len)

# g = sns.countplot(x=texts_len)
print('Mean length of the texts:', np.mean(texts_len))

"""## Purify & Shffle the DataFrame"""

from sklearn.utils import shuffle

# Purify
df = df.iloc[:,[0, -1]]

# Shuffle
df = shuffle(df).reset_index(drop=True)

display(df)

"""## Split Data into Training, Validation, Test"""

train_val_df = df.sample(frac = 0.8)
test_df = df.drop(train_val_df.index)

train_df = train_val_df.sample(frac = 0.8)
val_df = train_val_df.drop(train_df.index)

# Reset Index
train_df = train_df.reset_index(drop=True)
val_df = val_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

print('trainset size:', train_df.shape)
print('valset size:', val_df.shape)
print('testset size:', test_df.shape)

"""## Dataframe to csv"""

train_df.to_csv('train.tsv', sep='\t', index=False)
val_df.to_csv('val.tsv', sep='\t', index=False)
test_df.to_csv('test.tsv', sep='\t', index=False)

"""## Concatenate all dataframe"""

df = pd.concat([train_df, val_df, test_df])
df

"""## Performing Data Cleaning"""

import nltk
# Downloading Stopwords
nltk.download("stopwords")

# Obtaining Additional Stopwords From nltk
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])

# Removing Stopwords And Remove Words With 2 Or Less Characters
def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:
            result.append(token)

    return result

!pip install gensim

!pip install numpy==1.25.2
!pip install --upgrade gensim

import gensim

# Applying The Function To The Dataframe
df['clean'] = df['title'].apply(preprocess)

"""## Obtaining The Total Words Present In The Dataset"""

list_of_words = []
for i in df.clean:
    for j in i:
        list_of_words.append(j)

total_words = len(list(set(list_of_words)))
total_words

"""## Preparing The Data By Performing Tokenization And Padding"""

from nltk import word_tokenize

from tensorflow.keras.preprocessing.text import one_hot, Tokenizer

# Creating A Tokenizer To Tokenize The Words And Create Sequences Of Tokenized Words
tokenizer = Tokenizer(num_words = total_words)
tokenizer.fit_on_texts(train_df['title'])

train_sequences = tokenizer.texts_to_sequences(train_df['title'])
val_sequences = tokenizer.texts_to_sequences(val_df['title'])
test_sequences = tokenizer.texts_to_sequences(test_df['title'])

from tensorflow.keras.preprocessing.sequence import pad_sequences

# Adding Padding
padded_train = pad_sequences(train_sequences,maxlen = 42, padding = 'post', truncating = 'post')
padded_val = pad_sequences(val_sequences,maxlen = 42, padding = 'post', truncating = 'post')
padded_test = pad_sequences(test_sequences,maxlen = 42, padding = 'post', truncating = 'post')

"""# LSTM

## Building And Training LSTM Model
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout, BatchNormalization

## Creating model Using LSTM
embedding_vector_features=40
model=Sequential()
model.add(Embedding(total_words,embedding_vector_features,input_length=20))
model.add(Dropout(0.3))
model.add(LSTM(100))
model.add(Dropout(0.3))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())

y_train = np.asarray(train_df['true'])
y_val = np.asarray(val_df['true'])

# Training the model
model.fit(padded_train, y_train, batch_size = 64, validation_data=(padded_val, y_val), epochs = 3)

"""## Assessing Trained Model Performance"""

# Making prediction
prediction = np.argmax(model.predict(padded_test), axis=-1)

# Getting The Accuracy
from sklearn.metrics import accuracy_score
y_test = np.asarray(test_df['true'])

accuracy = accuracy_score(list(y_test), prediction)

print("LSTM Model Accuracy : ", accuracy)


# Getting The Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(list(y_test), prediction)
plt.figure(figsize = (6, 6))
sns.heatmap(cm, annot = True)

from sklearn.metrics import classification_report
print(classification_report(y_test, prediction))

# SVM Model Implementation

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(train_df['title'])
X_val_tfidf = vectorizer.transform(val_df['title'])
X_test_tfidf = vectorizer.transform(test_df['title'])

# SVM Model
svm_model = SVC(kernel='linear')
svm_model.fit(X_train_tfidf, train_df['true'])

# Predictions
svm_predictions = svm_model.predict(X_test_tfidf)

# Accuracy
accuracy = accuracy_score(test_df['true'], svm_predictions)
print("SVM Model Accuracy: ", accuracy)

# Confusion Matrix
cm = confusion_matrix(test_df['true'], svm_predictions)
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True)

# Classification Report
print(classification_report(test_df['true'], svm_predictions))

# Naive Bayes Model Implementation

from sklearn.naive_bayes import MultinomialNB

# Naive Bayes Model
nb_model = MultinomialNB()
nb_model.fit(X_train_tfidf, train_df['true'])

# Predictions
nb_predictions = nb_model.predict(X_test_tfidf)

# Accuracy
accuracy = accuracy_score(test_df['true'], nb_predictions)
print("Naive Bayes Model Accuracy: ", accuracy)

# Confusion Matrix
cm = confusion_matrix(test_df['true'], nb_predictions)
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True)

# Classification Report
print(classification_report(test_df['true'], nb_predictions))

# Random Forest Model Implementation

from sklearn.ensemble import RandomForestClassifier

# Random Forest Model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_tfidf, train_df['true'])

# Predictions
rf_predictions = rf_model.predict(X_test_tfidf)

# Accuracy
accuracy = accuracy_score(test_df['true'], rf_predictions)
print("Random Forest Model Accuracy: ", accuracy)

# Confusion Matrix
cm = confusion_matrix(test_df['true'], rf_predictions)
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True)

# Classification Report
print(classification_report(test_df['true'], rf_predictions))

"""# BERT"""

!pip install transformers

import torch
from transformers import BertTokenizer

PRETRAINED_MODEL_NAME = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)

"""## Load Dataset Class"""

from torch.utils.data import Dataset

class FakeNewsDataset(Dataset):
    def __init__(self, mode, tokenizer):
        assert mode in ['train', 'val', 'test']
        self.mode = mode
        self.df = pd.read_csv(mode + '.tsv', sep='\t').fillna("")
        self.len = len(self.df)
        self.tokenizer = tokenizer  # BERT tokenizer

    def __getitem__(self, idx):
        if self.mode == 'test':
            statement, label = self.df.iloc[idx, :].values
            label_tensor = torch.tensor(label)
        else:
            statement, label = self.df.iloc[idx, :].values
            label_tensor = torch.tensor(label)


        word_pieces = ['[CLS]']
        statement = self.tokenizer.tokenize(statement)
        word_pieces += statement + ['[SEP]']
        len_st = len(word_pieces)

        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)
        tokens_tensor = torch.tensor(ids)


        segments_tensor = torch.tensor([0] * len_st, dtype=torch.long)

        return (tokens_tensor, segments_tensor, label_tensor)

    def __len__(self):
        return self.len


# Initialize Datasets for Transformation
trainset = FakeNewsDataset('train', tokenizer=tokenizer)
valset = FakeNewsDataset('val', tokenizer=tokenizer)
testset = FakeNewsDataset('test', tokenizer=tokenizer)

print('trainset size:' ,trainset.__len__())
print('valset size:',valset.__len__())
print('testset size: ',testset.__len__())

"""## Sampling and Observing Tensors"""

sample_idx = 0

statement, label = trainset.df.iloc[sample_idx].values
tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]
tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())
combined_text = " ".join(tokens)
print(f"""
original_statement:
{statement}

tokens:
{tokens}

label: {label}

--------------------

tokens_tensor:
{tokens_tensor}

segments_tensor:
{segments_tensor}

label_tensor:
{label_tensor}

""")

"""## Reforming the Dataset to Fit the Model"""

from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence

def create_mini_batch(samples):
    tokens_tensors = [s[0] for s in samples]
    segments_tensors = [s[1] for s in samples]

    if samples[0][2] is not None:
        label_ids = torch.stack([s[2] for s in samples])
    else:
        label_ids = None

    # Zero Padding
    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)
    segments_tensors = pad_sequence(segments_tensors, batch_first=True)


    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)
    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)

    return tokens_tensors, segments_tensors, masks_tensors, label_ids

BATCH_SIZE = 16
trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)
valloader = DataLoader(valset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)
testloader = DataLoader(testset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch)

data = next(iter(trainloader))

tokens_tensors, segments_tensors, masks_tensors, label_ids = data

print(f"""
tokens_tensors.shape   = {tokens_tensors.shape}
{tokens_tensors}
------------------------
segments_tensors.shape = {segments_tensors.shape}
{segments_tensors}
------------------------
masks_tensors.shape    = {masks_tensors.shape}
{masks_tensors}
------------------------
label_ids.shape        = {label_ids.shape}
{label_ids}
""")

"""## Model Construction"""

from transformers import BertForSequenceClassification
from IPython.display import display, clear_output

PRETRAINED_MODEL_NAME = "bert-base-uncased"
NUM_LABELS = 2

model = BertForSequenceClassification.from_pretrained(
    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)

clear_output()

print("""
name             module
-----------------------""")
for name, module in model.named_children():
    if name == "bert":
        for n, _ in module.named_children():
            print(f"{name}:{n}")
    else:
        print("{:16} {}".format(name, module))

model.config

"""# Fine-Tuning of BERT"""

# %%time
from sklearn.metrics import accuracy_score
from tqdm.notebook import tqdm

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("device:", device)
model = model.to(device)

model.train()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
NUM_EPOCHS = 3

for epoch in range(NUM_EPOCHS):
    train_loss = 0.0
    train_acc = 0.0

    loop = tqdm(trainloader)
    for batch_idx, data in enumerate(loop):
        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]


        optimizer.zero_grad()

        outputs = model(input_ids=tokens_tensors,
                        token_type_ids=segments_tensors,
                        attention_mask=masks_tensors,
                        labels=labels)

        loss = outputs[0]
        loss.backward()
        optimizer.step()

        logits = outputs[1]
        _, pred = torch.max(logits.data, 1)
        train_acc = accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())

        # batch loss
        train_loss += loss.item()

        # if batch_idx == len(trainloader)-1:
        #     _, acc = get_predictions(model, trainloader, compute_acc=True)

        loop.set_description(f"Epoch [{epoch+1}/{NUM_EPOCHS}]")
        loop.set_postfix(acc = train_acc, loss = train_loss)

"""## Save Model"""

torch.save(model, './best_model.pth')
print('Model saved!')

"""## Load Model"""

# model = torch.load('./best_model.pth')
# model = model.to(device)

"""# Test"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

true=[]
predictions=[]
with torch.no_grad():
    model.eval()
    for data in testloader:
        if next(model.parameters()).is_cuda:
            data = [t.to(device) for t in data if t is not None]

        tokens_tensors, segments_tensors, masks_tensors = data[:3]
        test_outputs = model(input_ids=tokens_tensors,
                    token_type_ids=segments_tensors,
                    attention_mask=masks_tensors)

        logits = test_outputs[0]
        _, pred = torch.max(logits.data, 1)

        labels = data[3]
        true.extend(labels.cpu().tolist())
        predictions.extend(pred.cpu().tolist())


cm = confusion_matrix(true, predictions, labels=[1, 0], normalize='pred')
print(cm)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])
disp.plot()

print('Acc: ', accuracy_score(predictions,true))

df = pd.DataFrame({"pred_label": predictions})

df_pred = pd.concat([testset.df.loc[:, ['title']],
                          testset.df.loc[:, ['true']],
                          df.loc[:, 'pred_label']], axis=1)
# df_pred.to_csv('bert_1_prec_training_samples.csv', index=False)
df_pred

print(classification_report(df_pred.true, df_pred.pred_label))

"""## Insight on Wrong Classification"""

wrong_df = df_pred[df_pred.true != df_pred.pred_label]
sns.countplot(y="true", palette="coolwarm", data=wrong_df).set_title('Wrong Classification Result Real/Fake Distribution')
plt.show()

wrong_titles = df_pred[df_pred.true != df_pred.pred_label].title.values
wrong_titles

# Test

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

true = []
predictions = []
with torch.no_grad():
    model.eval()
    for data in testloader:
        if next(model.parameters()).is_cuda:
            data = [t.to(device) for t in data if t is not None]

        tokens_tensors, segments_tensors, masks_tensors = data[:3]
        test_outputs = model(input_ids=tokens_tensors,
                             token_type_ids=segments_tensors,
                             attention_mask=masks_tensors)

        logits = test_outputs[0]

        _, preds = torch.max(logits, dim=1)  # Get predicted class labels
        # Ensure true labels are added as individual elements, not arrays
        true.extend(data[3].cpu().numpy().tolist())  # True labels
        predictions.extend(preds.cpu().numpy().tolist())  # Predicted labels

# Confusion Matrix and Accuracy
cm = confusion_matrix(true, predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')
plt.show()

accuracy = accuracy_score(true, predictions)
print(f'Accuracy: {accuracy * 100:.2f}%')

import torch
from transformers import BertTokenizer
from torch.utils.data import DataLoader
from transformers import BertForSequenceClassification
from torch.nn.utils.rnn import pad_sequence  # Import pad_sequence

# Load the saved model
# model = torch.load('/content/best_model.pth')
# model = model.to(device)
# Load the saved model with `safe_globals` context manager
with torch.serialization.safe_globals([BertForSequenceClassification]):  # Allowlist global
    model = torch.load('/content/best_model.pth', weights_only=False)

model = model.to(device)

# Load the tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Sample text input for prediction
sample_texts = ["This is a sample news article about politics.", "Fake news is spreading on social media."]

# Tokenize the input
def preprocess_text(texts):
    word_pieces = ['[CLS]']
    tokenized_texts = [tokenizer.tokenize(text) for text in texts]
    word_pieces += [item for sublist in tokenized_texts for item in sublist] + ['[SEP]']
    input_ids = tokenizer.convert_tokens_to_ids(word_pieces)
    return input_ids

input_ids = [preprocess_text([text]) for text in sample_texts]

# Pad sequences to the same length
input_ids = pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=0).to(device) # Pad sequences

# Prepare DataLoader for prediction
class PredictionDataset(torch.utils.data.Dataset):
    def __init__(self, input_ids):
        self.input_ids = input_ids

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx]

predict_dataset = PredictionDataset(input_ids)
predict_loader = DataLoader(predict_dataset, batch_size=1)

# Make predictions
model.eval()
predictions = []
with torch.no_grad():
    for data in predict_loader:
        tokens_tensor = data.to(device)
        outputs = model(input_ids=tokens_tensor)
        logits = outputs[0]
        _, predicted_class = torch.max(logits, dim=1)
        predictions.append(predicted_class.cpu().item())

# Display the predictions
for i, text in enumerate(sample_texts):
    print(f"Text: {text}")
    print(f"Prediction: {'Fake' if predictions[i] == 0 else 'True'}\n")

"""**Building Streamlit app**"""

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import torch
# from transformers import BertTokenizer, BertForSequenceClassification
# import streamlit as st
# 
# # Set the device (GPU or CPU)
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# 
# # Load the entire saved model (no need for state_dict here)
# model = torch.load('/content/best_model.pth', weights_only=False)
# model = model.to(device)
# 
# # Load the tokenizer
# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# 
# # Streamlit App UI
# st.title('Fake News Detection')
# st.write("Enter a news article or text below:")
# 
# # Text input box
# input_text = st.text_area("Input Text", "Enter Text.")
# 
# if st.button('Predict'):
#     if not input_text.strip():  # Check if input is empty or only whitespace
#         st.error("Input Cannot be empty")
#     else:
#         # Tokenize the input text
#         inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors="pt", max_length=512)
#         input_ids = inputs['input_ids'].to(device)
#         attention_mask = inputs['attention_mask'].to(device)
# 
#     # Make predictions
#     model.eval()
#     with torch.no_grad():
#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)
#         logits = outputs.logits
#         _, predicted_class = torch.max(logits, dim=1)
#         prediction = 'Fake' if predicted_class.item() == 0 else 'True'
# 
#     # Display prediction
#     st.write(f"Prediction: {prediction}")

! pip install streamlit -q

!wget -q -O - ipv4.icanhazip.com

!streamlit run app.py & npx localtunnel --port 8501

!pip install flask transformers torch